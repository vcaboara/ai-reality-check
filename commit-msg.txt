[AI] feat: update model display with VRAM requirements

Changes:
- Fixed default model fallback to qwen2.5-coder:7b (was llama3.2:3b)
- Added VRAM requirements to UI header display
- Created comprehensive model comparison table in DOCKER.md
- Shows model name with RAM requirement: "qwen2.5-coder:7b (8GB RAM)"

Model VRAM Reference:
- phi3:mini - 2GB
- llama3.2:3b - 4GB  
- llama3.1:8b - 8GB
- qwen2.5-coder:7b - 8GB (recommended)
- qwen2.5-coder:14b - 16GB
- qwen2.5-coder:32b - 32GB+

Users can now see at a glance what hardware their model requires.

---
AI-Generated-By: GitHub Copilot (Claude Sonnet 4.5)
