services:
  # Ollama service (optional - uncomment if not running Ollama locally)
  # If you have Ollama running on your host, leave this commented out
  # and use OLLAMA_BASE_URL=http://host.docker.internal:11434 in web service
  
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ai-reality-check-ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   environment:
  #     - OLLAMA_HOST=0.0.0.0
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  web:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai-reality-check-web
    ports:
      - "5000:5000"
    volumes:
      - ./data:/app/data
      - ./config:/app/config
    environment:
      # Use host.docker.internal to connect to Ollama running on host machine
      # Or use http://ollama:11434 if using the containerized Ollama service above
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5-coder:7b}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - FLASK_ENV=${FLASK_ENV:-production}
      - PORT=5000
    restart: unless-stopped

volumes:
  ollama_data:
